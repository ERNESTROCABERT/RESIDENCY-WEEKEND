{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87c46d8-f7fc-420a-87d9-edec2edcdc5d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\alex-\\anaconda3\\lib\\site-packages (0.11.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\alex-\\anaconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: faiss-cpu==1.7.4 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alex-\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: yfinance in c:\\users\\alex-\\anaconda3\\lib\\site-packages (0.2.36)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from pdfplumber) (10.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (41.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (1.5.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (4.9.3)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (2023.3.post1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (2.4.0)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (3.17.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (4.12.2)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\alex-\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber transformers faiss-cpu==1.7.4 tqdm yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b338f4c6-d9b5-4ab1-9e94-9e2946d965de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba93be1-5034-4b08-b688-b72271ad9511",
   "metadata": {},
   "source": [
    "## 1. Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e708f7c-aeaf-4aa5-9e65-471ab7f7f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_path = \"Documents\\\\\"\n",
    "# files = os.listdir(documents_path)\n",
    "\n",
    "# docs = {}\n",
    "# for file in files:\n",
    "#     file_path = documents_path + file\n",
    "#     with pdfplumber.open(file_path) as pdf:\n",
    "#         all_text = ''\n",
    "#         # Loop through each page in the document\n",
    "#         for page in pdf.pages:\n",
    "#             # Extract text from the page\n",
    "#             all_text += page.extract_text()           \n",
    "#     docs[file] = all_text\n",
    "\n",
    "# docs = {k.replace(\".pdf\", \"\"): v for k, v in docs.items()}\n",
    "\n",
    "# # Saving the dictionary to a pickle file\n",
    "# with open('Docs.pkl', 'wb') as f:\n",
    "#     pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57207c0-d36e-47e9-869f-f257d25b74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the documents from a pickle file\n",
    "with open('Docs.pkl', 'rb') as f:\n",
    "    docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a100be4a-d220-44dc-a541-182445347477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, tokenizer, max_tokens=512, chunk_size=100, overlap=25):\n",
    "\n",
    "    tokens = tokenizer.encode(text)\n",
    "        \n",
    "    # List to hold the chunks\n",
    "    chunks = []\n",
    "    \n",
    "    # Start index\n",
    "    start = 0\n",
    "    \n",
    "    while start < (len(tokens) - overlap):\n",
    "        # End index of the chunk (consider overlap)\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Ensure we don't exceed the max token limit\n",
    "        if end > len(tokens):\n",
    "            end = len(tokens)\n",
    "        \n",
    "        # Create the chunk (from start to end index)\n",
    "        chunk = tokens[start:end]\n",
    "        \n",
    "        # Convert tokens back to text\n",
    "        chunk_text = tokenizer.decode(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        # Update the start index (add overlap)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69cd13e8-a644-4da8-b84c-eba60946dacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMZN...\n",
      "GOOGL...\n",
      "META...\n",
      "MSFT...\n",
      "NVDA...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the FinBERT tokenizer\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_tokens = 512  # BERT model max token limit\n",
    "chunk_size = 100  # Target chunk size (tokens)\n",
    "overlap = 25  # Overlap between chunks\n",
    "\n",
    "paragraphs = {}\n",
    "for stock, text in docs.items():\n",
    "    print(f\"{stock}...\")\n",
    "    paragraphs[stock] = split_text_into_chunks(text, tokenizer, max_tokens, chunk_size, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eccc2801-76d4-4c9b-a721-375cb3f6117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMZN: 855 paragraphs\n",
      "GOOGL: 975 paragraphs\n",
      "META: 1345 paragraphs\n",
      "MSFT: 1303 paragraphs\n",
      "NVDA: 925 paragraphs\n"
     ]
    }
   ],
   "source": [
    "for stock in paragraphs.keys():\n",
    "    print(f\"{stock}: {len(paragraphs[stock])} paragraphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08435509-8b62-4bf3-a988-23e271d67045",
   "metadata": {},
   "source": [
    "## 2. Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d3658d-49ea-4e55-b4cf-471222dda38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "# Load FinBERT model\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e56a82a2-6677-4ec0-94f3-dea5abb2ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks, model, tokenizer):\n",
    "    embeddings = []\n",
    "    \n",
    "    for chunk in tqdm(chunks):\n",
    "        # Tokenize the chunk\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Ensure we're working with the model in evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Get the embeddings from the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # The last hidden state gives the contextualized embeddings\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        # Mean pooling over the entire sequence\n",
    "        chunk_embeddings = last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.append(chunk_embeddings)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5bbf70b-fd7c-4ceb-9e8c-0c9427f2e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = {}\n",
    "# for stock, chunks in paragraphs.items():\n",
    "#     print(f\"{stock}...\")\n",
    "#     vectors[stock] = generate_embeddings(chunks, model, tokenizer)\n",
    "\n",
    "# # Saving the dictionary to a pickle file\n",
    "# with open('Vectors.pkl', 'wb') as f:\n",
    "#     pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913a0da-e153-4b10-801a-6858926d8a12",
   "metadata": {},
   "source": [
    "## 3. Building the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64fefea6-8f60-4bcd-bfee-058749af2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dictionary from a pickle file\n",
    "with open('Vectors.pkl', 'rb') as f:\n",
    "    vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e99aefa8-e5d3-4889-a7ef-e310b6b21785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 28.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add metadata to vectors\n",
    "\n",
    "metadata_vectors = []\n",
    "for stock, company_vectors in tqdm(vectors.items()):\n",
    "    # Fetch the company name using yfinance\n",
    "    ticker = yf.Ticker(stock)\n",
    "    \n",
    "    for i, vector in enumerate(company_vectors):\n",
    "        # Define metadata for each vector\n",
    "        vector_metadata = {\n",
    "            \"vector\": vector,\n",
    "            \"metadata\": {\n",
    "                \"vector_id\": f\"{stock}-{i}\",\n",
    "                \"ticker\": stock,\n",
    "                \"paragraph_number\": i,\n",
    "                \"paragraph_text\": paragraphs[stock][i]  # Assuming 'paragraphs' is defined elsewhere\n",
    "            }\n",
    "        }\n",
    "        # Append the structured data to the list\n",
    "        metadata_vectors.append(vector_metadata)\n",
    "\n",
    "# Company identifier\n",
    "company_vector = []\n",
    "for i in range (0, len(metadata_vectors)):\n",
    "    company_vector.append(metadata_vectors[i]['metadata']['ticker'])\n",
    "\n",
    "vectors = [entry['vector'] for entry in metadata_vectors]\n",
    "vectors = np.array(vectors).reshape(len(vectors), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "472617df-d9c3-413b-84b5-8f94da308fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name_map = {\n",
    "    'GOOGL': ['google', 'alphabet', 'youtube', 'android'],\n",
    "     'AMZN': ['amazon', 'aws'],\n",
    "     'NVDA': ['nvidia'],\n",
    "     'MSFT': ['Microsoft', 'Windows', 'Azure'],\n",
    "     'META': ['Meta', 'Facebook', 'Instagram']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df421f07-7968-4cad-874b-4a39b3c34067",
   "metadata": {},
   "source": [
    "## 4. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3a299f9-d76a-4699-b445-6eda734d9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = [\"\"\"\n",
    "Talk about Microsoft stock-based compensation\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7684ec9-cce9-4eb6-9eb4-db8427e62e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 3937 with distance 0.6368639469146729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# If there are any mentions to a specific company, narrow the similarity search to the 10K documents of those companies only\n",
    "focus_tickers = []\n",
    "for ticker, words in company_name_map.items():\n",
    "    for word in words:\n",
    "        if word.lower() in user_query[0].lower():\n",
    "            focus_tickers.append(ticker)\n",
    "            break\n",
    "focused_vectors_idx = [i for i, ticker in enumerate(company_vector) if ticker in focus_tickers]\n",
    "vectors_focused = vectors[focused_vectors_idx]\n",
    "vectors_focused = []\n",
    "if len(vectors_focused) > 0:\n",
    "    # Normalize the vectors to unit length\n",
    "    normalized_vectors = vectors_focused / np.linalg.norm(vectors_focused, axis=1, keepdims=True)\n",
    "else:\n",
    "    normalized_vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "\n",
    "# Encode the query to get embeddings\n",
    "query_vector = generate_embeddings(user_query, model, tokenizer)[0].reshape(1, -1)\n",
    "# Normalize the query vector\n",
    "query_vector = query_vector / np.linalg.norm(query_vector)\n",
    "\n",
    "# Create a FAISS index for the normalized vectors\n",
    "index = faiss.IndexFlatIP(normalized_vectors.shape[1])  # Use Inner Product (IP), which is equivalent to cosine similarity after normalization\n",
    "index.add(normalized_vectors)\n",
    "normalized_vectors.shape\n",
    "\n",
    "# Perform the nearest neighbor search\n",
    "distances, indices = index.search(query_vector, k=1)  # k=n for top N nearest neighbors\n",
    "\n",
    "for idx, dist in zip(indices[0], distances[0]):\n",
    "    print(f\"Paragraph {idx} with distance {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98ec891d-4e1c-459b-b38c-8f452dc5a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and savings plans we grant stock - based compensation to employees and directors. awards that expire or are canceled without delivery of shares generally become available for issuance under the plans. we issue new shares of microsoft common stock to satisfy vesting of awards granted under our stock plans. we also have an espp for all eligible employees. stock - based compensation expense and related income tax benefits were as follows : ( in millions ) year ended june 30, 2024 2023 2022 stock - based compensation expense $ 10, 734 $ 9\n"
     ]
    }
   ],
   "source": [
    "relevant_texts = \"\"\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    text = metadata_vectors[idx]['metadata']['paragraph_text']\n",
    "    if i == len(indices[0]) - 1:\n",
    "        relevant_texts += text\n",
    "    else:\n",
    "        relevant_texts += text + \"\\n\\n\"\n",
    "print(relevant_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a7933-042b-47fc-8829-4b4233b3acfe",
   "metadata": {},
   "source": [
    "## 5. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcebd15f-f1dd-4948-8daa-9e23b7348af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load DistilGPT-2 model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a4cfc-e120-4d5a-8809-2350eb8289ea",
   "metadata": {},
   "source": [
    "#### RAG Prompt (With Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "441a2b08-b69e-4269-815f-0f051b701836",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\\n\\n Context:\\n\\n\" \"\"\" + relevant_texts + \"\"\"\"\\n\"\"\" + user_query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb742172-3c9c-49bd-b844-8fe2c45a7d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Context:\n",
      "\n",
      "\" and savings plans we grant stock - based compensation to employees and directors. awards that expire or are canceled without delivery of shares generally become available for issuance under the plans. we issue new shares of microsoft common stock to satisfy vesting of awards granted under our stock plans. we also have an espp for all eligible employees. stock - based compensation expense and related income tax benefits were as follows : ( in millions ) year ended june 30, 2024 2023 2022 stock - based compensation expense $ 10, 734 $ 9\"\n",
      "\n",
      "Talk about Microsoft stock-based compensation\n",
      "If you are a Microsoft employee, you may be eligible to receive a share of the company’s stock. If you have not received the share, the stock will not be offered to you. However, if you do not receive the shares, it will be subject to the terms and conditions of this agreement.\n",
      "How do you get your share?\n",
      "We offer a variety of options for you to choose from. For example, we offer the option of a stock of $1,000 to $2,500, or $3,200, depending on the type of stock you want to buy. We also offer an option for $10, $15, and $20, as well as $25, which you can use to pay off your debt. You can also use this option to set up a new account for your existing account. The option is available at www.microsoft.com/en-us/privacy-for-employees-and-counsel-of-the-company/\n",
      "For more information, please visit: http://en.wikipedia.org/wiki/How-to-get-your-share\n",
      "About Microsoft Corporation\n",
      "Microsoft Corporation is a global leader in the Internet of Things (IoT) and is the world's leading provider of computing, entertainment, information and services. Microsoft is headquartered in Redmond, Washington, D.C., and serves as President, CEO, Chief Executive Officer, President and Chief Operating Officer of Microsoft.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=500,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id to eos_token_id for consistency\n",
    "    temperature=0.7,  # Control randomness\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    num_beams=10,  # Beam search for better answers\n",
    "    no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode and print the answer\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09275218-f2a3-4844-a729-e39db89d8d6c",
   "metadata": {},
   "source": [
    "#### Non-RAG Prompt (NO Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f66fc4c7-04f1-4be7-aff1-7abc6f68dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  user_query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f653eb6-418a-43bc-af6b-388aa0e7be54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Talk about Microsoft stock-based compensation\n",
      "\n",
      "What do you think?\n",
      "Share your thoughts in the comments below.\n",
      "This article was originally published on The Conversation. Read the original article.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=500,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id to eos_token_id for consistency\n",
    "    temperature=0.7,  # Control randomness\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    "    num_beams=10,  # Beam search for better answers\n",
    "    no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode and print the answer\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
